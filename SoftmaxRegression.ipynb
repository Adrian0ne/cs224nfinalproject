{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's create a fake data tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "seq_len = 4\n",
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "inputs = torch.randn((batch_size, seq_len, hidden_size)) # matrix of size (batch_size, seq_len, hidden_size)\n",
    "idx_targets = torch.tensor([0, 1, 3]) # matrix of size (batch_size) where entry is class idx\n",
    "print(inputs.shape, idx_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, our index softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxRegression import IdxSoftmaxRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "device = \"cpu\" # else \"cuda:0\"\n",
    "max_epoch = 2000\n",
    "print_every = 100\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.9858344793319702\n",
      "epoch 200 loss 0.7027537226676941\n",
      "epoch 300 loss 0.5020290613174438\n",
      "epoch 400 loss 0.3692546784877777\n",
      "epoch 500 loss 0.2781364917755127\n",
      "epoch 600 loss 0.21403783559799194\n",
      "epoch 700 loss 0.16786591708660126\n",
      "epoch 800 loss 0.13383346796035767\n",
      "epoch 900 loss 0.10820576548576355\n",
      "epoch 1000 loss 0.08852928876876831\n",
      "epoch 1100 loss 0.07316025346517563\n",
      "epoch 1200 loss 0.060973044484853745\n",
      "epoch 1300 loss 0.05118098109960556\n",
      "epoch 1400 loss 0.04322313144803047\n",
      "epoch 1500 loss 0.03669150546193123\n",
      "epoch 1600 loss 0.03128419816493988\n",
      "epoch 1700 loss 0.026774315163493156\n",
      "epoch 1800 loss 0.022988714277744293\n",
      "epoch 1900 loss 0.019793258979916573\n",
      "epoch 2000 loss 0.017082812264561653\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = IdxSoftmaxRegression(seq_len, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(lr))\n",
    "\n",
    "epoch = 0\n",
    "while epoch < max_epoch:\n",
    "    epoch += 1\n",
    "    loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(batch_size):\n",
    "        example = inputs[i, :, :].unsqueeze(0)\n",
    "        target = idx_targets[i].unsqueeze(0)\n",
    "        \n",
    "        loss = model.train_forward(example, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch%print_every==0:\n",
    "        print(\"epoch {} loss {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.6258e-01, 3.6998e-02, 8.5927e-06, 4.0928e-04],\n",
       "        [2.9356e-05, 9.8474e-01, 8.4081e-03, 6.8241e-03],\n",
       "        [4.6515e-04, 9.9967e-03, 6.4669e-03, 9.8307e-01]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict_proba(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then let's do a softmax for whether or not there is an answer at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxRegression import ImpossibleSoftmaxRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "inputs = torch.randn((batch_size, seq_len, hidden_size)) # matrix of size (batch_size, seq_len, hidden_size)\n",
    "is_impossible_targets = torch.tensor([0, 1, 0], dtype=torch.float) # matrix of size (batch_size) where entry is binary whether there is an answer\n",
    "print(inputs.shape, is_impossible_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # else \"cuda:0\"\n",
    "lr = 1e-3\n",
    "max_grad_norm = 1.0\n",
    "max_epoch = 2000\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.14965412020683289\n",
      "epoch 200 loss 0.0693732351064682\n",
      "epoch 300 loss 0.03901948407292366\n",
      "epoch 400 loss 0.024781912565231323\n",
      "epoch 500 loss 0.017031462863087654\n",
      "epoch 600 loss 0.012340199202299118\n",
      "epoch 700 loss 0.009279616177082062\n",
      "epoch 800 loss 0.0071703349240124226\n",
      "epoch 900 loss 0.005655335728079081\n",
      "epoch 1000 loss 0.004531742073595524\n",
      "epoch 1100 loss 0.003677146742120385\n",
      "epoch 1200 loss 0.0030137368012219667\n",
      "epoch 1300 loss 0.0024901016149669886\n",
      "epoch 1400 loss 0.0020711994729936123\n",
      "epoch 1500 loss 0.0017322394996881485\n",
      "epoch 1600 loss 0.0014553522923961282\n",
      "epoch 1700 loss 0.0012273568427190185\n",
      "epoch 1800 loss 0.0010384346824139357\n",
      "epoch 1900 loss 0.000880927313119173\n",
      "epoch 2000 loss 0.0007489743293263018\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = ImpossibleSoftmaxRegression(seq_len, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(lr))\n",
    "\n",
    "epoch = 0\n",
    "while epoch < max_epoch:\n",
    "    epoch += 1\n",
    "    loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(batch_size):\n",
    "        example = inputs[i, :, :].unsqueeze(0)\n",
    "        target = is_impossible_targets[i].unsqueeze(0)\n",
    "        \n",
    "        loss = model.train_forward(example, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch%print_every==0:\n",
    "        print(\"epoch {} loss {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1660e-04, 9.9878e-01, 7.4828e-04], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict_proba(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0.], grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's also look at using our multi softmax class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "inputs = torch.randn((batch_size, seq_len, hidden_size)) # matrix of size (batch_size, seq_len, hidden_size)\n",
    "start_idx_targets = torch.tensor([0, 1, 2]) # matrix of size (batch_size) where entry is class idx\n",
    "stop_idx_targets = torch.tensor([1, 1, 3]) # matrix of size (batch_size) where entry is class idx\n",
    "is_impossible_targets = torch.tensor([0, 1, 0], dtype=torch.float) # matrix of size (batch_size) where entry is binary whether answering is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxRegression import MultiSoftmaxRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiSoftmaxRegression(seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss start 1.32, stop 1.07, impossible 0.44\n",
      "epoch 200, loss start 1.10, stop 0.88, impossible 0.19\n",
      "epoch 300, loss start 0.92, stop 0.73, impossible 0.08\n",
      "epoch 400, loss start 0.77, stop 0.62, impossible 0.04\n",
      "epoch 500, loss start 0.64, stop 0.53, impossible 0.02\n",
      "epoch 600, loss start 0.53, stop 0.46, impossible 0.01\n",
      "epoch 700, loss start 0.44, stop 0.40, impossible 0.00\n",
      "epoch 800, loss start 0.36, stop 0.35, impossible 0.00\n",
      "epoch 900, loss start 0.29, stop 0.31, impossible 0.00\n",
      "epoch 1000, loss start 0.24, stop 0.27, impossible 0.00\n",
      "epoch 1100, loss start 0.20, stop 0.24, impossible 0.00\n",
      "epoch 1200, loss start 0.16, stop 0.21, impossible 0.00\n",
      "epoch 1300, loss start 0.13, stop 0.19, impossible 0.00\n",
      "epoch 1400, loss start 0.11, stop 0.17, impossible 0.00\n",
      "epoch 1500, loss start 0.09, stop 0.15, impossible 0.00\n",
      "epoch 1600, loss start 0.07, stop 0.13, impossible 0.00\n",
      "epoch 1700, loss start 0.06, stop 0.12, impossible 0.00\n",
      "epoch 1800, loss start 0.05, stop 0.11, impossible 0.00\n",
      "epoch 1900, loss start 0.04, stop 0.09, impossible 0.00\n",
      "epoch 2000, loss start 0.03, stop 0.08, impossible 0.00\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 2000\n",
    "print_every = 100\n",
    "epoch = 0\n",
    "device = \"cpu\"\n",
    "while epoch < max_epoch:\n",
    "    epoch += 1\n",
    "    start_idx_loss = model.train_step(inputs, start_idx_targets, 'start', device)\n",
    "    stop_idx_loss = model.train_step(inputs, stop_idx_targets, 'stop', device)\n",
    "    is_impossible_loss = model.train_step(inputs, is_impossible_targets, 'impossible', device)\n",
    "    \n",
    "    if epoch%print_every==0:\n",
    "        print(\"epoch {}, loss start {:.2f}, stop {:.2f}, impossible {:.2f}\".format(epoch, start_idx_loss, stop_idx_loss, is_impossible_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [-1., -1.],\n",
       "       [ 2.,  3.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(inputs, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's experiment with saving and loading our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make a directory 'softmax model' in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./sm_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiSoftmaxRegression(seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./sm_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [-1., -1.],\n",
       "       [ 2.,  3.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(inputs, device))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
