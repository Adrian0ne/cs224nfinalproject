{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's create a fake data tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "seq_len = 4\n",
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "data = torch.randn((batch_size, seq_len, hidden_size)) # matrix of size (batch_size, seq_len, hidden_size)\n",
    "targets = torch.tensor([0, 1, 3]) # matrix of size (batch_size) where entry is class idx\n",
    "print(data.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, our softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_size):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W = torch.nn.Linear(self.hidden_size, 1, bias=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        scores = self.W(input).squeeze(-1)\n",
    "        return scores\n",
    "    \n",
    "    def train_forward(self, input, target):\n",
    "        scores = self.forward(input)\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=-100)(scores, target)\n",
    "        return loss\n",
    "    \n",
    "    def predict_proba(self, input):\n",
    "        scores = self.forward(input)\n",
    "        p = F.softmax(scores, dim=-1)\n",
    "        return p\n",
    "    \n",
    "    def predict(self, input):\n",
    "        p = self.predict_proba(input)\n",
    "        _, preds = p.max(-1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "device = \"cpu\" # else \"cuda:0\"\n",
    "max_epoch = 2000\n",
    "print_every = 100\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.9858344793319702\n",
      "epoch 200 loss 0.7027537226676941\n",
      "epoch 300 loss 0.5020290613174438\n",
      "epoch 400 loss 0.3692546784877777\n",
      "epoch 500 loss 0.2781364917755127\n",
      "epoch 600 loss 0.21403783559799194\n",
      "epoch 700 loss 0.16786591708660126\n",
      "epoch 800 loss 0.13383346796035767\n",
      "epoch 900 loss 0.10820576548576355\n",
      "epoch 1000 loss 0.08852928876876831\n",
      "epoch 1100 loss 0.07316025346517563\n",
      "epoch 1200 loss 0.060973044484853745\n",
      "epoch 1300 loss 0.05118098109960556\n",
      "epoch 1400 loss 0.04322313144803047\n",
      "epoch 1500 loss 0.03669150546193123\n",
      "epoch 1600 loss 0.03128419816493988\n",
      "epoch 1700 loss 0.026774315163493156\n",
      "epoch 1800 loss 0.022988714277744293\n",
      "epoch 1900 loss 0.019793258979916573\n",
      "epoch 2000 loss 0.017082812264561653\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = SoftmaxRegression(seq_len, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=float(lr))\n",
    "\n",
    "epoch = 0\n",
    "while epoch < max_epoch:\n",
    "    epoch += 1\n",
    "    loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(batch_size):\n",
    "        example = data[i, :, :].unsqueeze(0)\n",
    "        target = targets[i].unsqueeze(0)\n",
    "        \n",
    "        loss = model.train_forward(example, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch%print_every==0:\n",
    "        print(\"epoch {} loss {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.6258e-01, 3.6998e-02, 8.5927e-06, 4.0928e-04],\n",
       "        [2.9356e-05, 9.8474e-01, 8.4081e-03, 6.8241e-03],\n",
       "        [4.6515e-04, 9.9967e-03, 6.4669e-03, 9.8307e-01]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict_proba(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(data))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
